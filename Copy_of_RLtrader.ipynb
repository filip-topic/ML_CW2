{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4L9MAoJXCCdB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datetime import datetime\n",
        "import itertools\n",
        "import argparse\n",
        "import re\n",
        "import os\n",
        "import pickle\n",
        "# packages that are used inside sklearn\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "r2u__9pCCVj_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Let's use AAPL (Apple), MSI (Motorola), SBUX (Starbucks)\n",
        "def get_data():\n",
        "  # returns a T x 3 list of stock prices\n",
        "  # each row is a different stock\n",
        "  # 0 = AAPL\n",
        "  # 1 = MSI\n",
        "  # 2 = SBUX\n",
        "  df = pd.read_csv('aapl_msi_sbux.csv')\n",
        "  return df.values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5VpQ5TULFGFa"
      },
      "outputs": [],
      "source": [
        "def get_scaler(env):\n",
        "  # return scikit-learn scaler object to scale the states\n",
        "  states = []\n",
        "  for _ in range(env.n_step):\n",
        "    action = np.random.choice(env.action_space)\n",
        "    state, reward, done, info = env.step(action)\n",
        "    states.append(state)\n",
        "    if done:\n",
        "      break\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  scaler.fit(states)\n",
        "  return scaler\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9xHiB1R4FMnu"
      },
      "outputs": [],
      "source": [
        "def maybe_make_dir(directory):\n",
        "  if not os.path.exists(directory):\n",
        "    os.makedirs(directory)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yQpsgYF-FOsj"
      },
      "outputs": [],
      "source": [
        "class LinearModel:\n",
        "  \"\"\" A linear regression model \"\"\"\n",
        "  def __init__(self, input_dim, n_action):\n",
        "    self.W = np.random.randn(input_dim, n_action) / np.sqrt(input_dim)\n",
        "    self.b = np.zeros(n_action)\n",
        "\n",
        "    # momentum terms\n",
        "    self.vW = 0\n",
        "    self.vb = 0\n",
        "\n",
        "    self.losses = []\n",
        "\n",
        "  def predict(self, X):\n",
        "    # make sure X is N x D (N sample size - how many obs we use (below it is N = 1), D state dimension - 7 in our case)\n",
        "    assert(len(X.shape) == 2) # assert is to chech if the condition is TRUE otherwise we have a warning error!\n",
        "    return X.dot(self.W) + self.b # this is N(=1) x D vector\n",
        "\n",
        "  def sgd(self, X, Y, learning_rate=0.01, momentum=0.9):\n",
        "    # make sure X is a matrix 1 x D (Q-values)\n",
        "    assert(len(X.shape) == 2)\n",
        "\n",
        "    # X is the current Q(s,:)\n",
        "    # Y is the target r + gamma max Q(s',:)\n",
        "    # the loss values are 2-D\n",
        "    # normally we would divide by N only\n",
        "    # but now we divide by N x K - K is num_action, i.e. the number of outputs\n",
        "    num_values = np.prod(Y.shape)\n",
        "\n",
        "    # do one step of gradient descent\n",
        "    # we multiply by 2 to get the exact gradient\n",
        "    # (not adjusting the learning rate)\n",
        "    # i.e. d/dx (x^2) --> 2x\n",
        "    Yhat = self.predict(X)\n",
        "    gW = 2 * X.T.dot(Yhat - Y) / num_values # here 2 is because of the derivative of the mean squared error \\sum_{k=1}^n_action (y-yhat)^2\n",
        "    gb = 2 * (Yhat - Y).sum(axis=0) / num_values\n",
        "\n",
        "    # update momentum terms\n",
        "    self.vW = momentum * self.vW - learning_rate * gW\n",
        "    self.vb = momentum * self.vb - learning_rate * gb\n",
        "\n",
        "    # update params\n",
        "    self.W += self.vW\n",
        "    self.b += self.vb\n",
        "\n",
        "    mse = np.mean((Yhat - Y)**2)\n",
        "    self.losses.append(mse)\n",
        "\n",
        "  def load_weights(self, filepath):\n",
        "    npz = np.load(filepath)\n",
        "    self.W = npz['W']\n",
        "    self.b = npz['b']\n",
        "\n",
        "  def save_weights(self, filepath):\n",
        "    np.savez(filepath, W=self.W, b=self.b)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_ysfQDTcFgq0"
      },
      "outputs": [],
      "source": [
        "class MultiStockEnv:\n",
        "  \"\"\"\n",
        "  A 3-stock trading environment.\n",
        "  State: vector of size 7 (n_stock * 2 + 1)\n",
        "    - # shares of stock 1 owned\n",
        "    - # shares of stock 2 owned\n",
        "    - # shares of stock 3 owned\n",
        "    - price of stock 1 (using daily close price)\n",
        "    - price of stock 2\n",
        "    - price of stock 3\n",
        "    - cash owned (can be used to purchase more stocks)\n",
        "  Action: categorical variable with 27 (3^3) possibilities\n",
        "    - for each stock, you can:\n",
        "    - 0 = sell\n",
        "    - 1 = hold\n",
        "    - 2 = buy\n",
        "  \"\"\"\n",
        "  def __init__(self, data, initial_investment=20000):\n",
        "    # data\n",
        "    self.stock_price_history = data\n",
        "    self.n_step, self.n_stock = self.stock_price_history.shape # table of n_days x 3 stocks\n",
        "\n",
        "    # instance attributes\n",
        "    self.initial_investment = initial_investment\n",
        "    self.cur_step = None\n",
        "    self.stock_owned = None\n",
        "    self.stock_price = None\n",
        "    self.cash_in_hand = None\n",
        "\n",
        "    self.action_space = np.arange(3**self.n_stock) # vector counting the number of possible actions: [0,1,2,...,26]\n",
        "\n",
        "    # action permutations\n",
        "    # returns a nested list with elements like:\n",
        "    # [[0,0,0]\n",
        "    # [0,0,1]\n",
        "    # [0,0,2]\n",
        "    # [0,1,0]\n",
        "    # [0,1,1]...\n",
        "    # [2,2,2]]\n",
        "    # etc.\n",
        "    # 0 = sell\n",
        "    # 1 = hold\n",
        "    # 2 = buy\n",
        "    self.action_list = list(map(list, itertools.product([0, 1, 2], repeat=self.n_stock)))\n",
        "\n",
        "    # calculate size of state\n",
        "    self.state_dim = self.n_stock * 2 + 1\n",
        "\n",
        "    self.reset()\n",
        "\n",
        "\n",
        "  def reset(self):\n",
        "    self.cur_step = 0\n",
        "    self.stock_owned = np.zeros(self.n_stock)\n",
        "    self.stock_price = self.stock_price_history[self.cur_step]\n",
        "    self.cash_in_hand = self.initial_investment\n",
        "    return self._get_obs()\n",
        "\n",
        "\n",
        "  def step(self, action):\n",
        "    assert action in self.action_space\n",
        "\n",
        "    # get current value before performing the action\n",
        "    prev_val = self._get_val()\n",
        "\n",
        "    # update price, i.e. go to the next day\n",
        "    self.cur_step += 1\n",
        "    self.stock_price = self.stock_price_history[self.cur_step]\n",
        "\n",
        "    # perform the trade\n",
        "    self._trade(action)\n",
        "\n",
        "    # get the new portfolio value after taking the action\n",
        "    cur_val = self._get_val()\n",
        "\n",
        "    # reward is the increase in porfolio value\n",
        "    reward = cur_val - prev_val\n",
        "\n",
        "    # done if we have run out of data\n",
        "    done = self.cur_step == self.n_step - 1\n",
        "\n",
        "    # store the current value of the portfolio here\n",
        "    info = {'cur_val': cur_val}\n",
        "\n",
        "    # output:\n",
        "    return self._get_obs(), reward, done, info\n",
        "\n",
        "\n",
        "  def _get_obs(self):\n",
        "    obs = np.empty(self.state_dim)\n",
        "    obs[:self.n_stock] = self.stock_owned\n",
        "    obs[self.n_stock:2*self.n_stock] = self.stock_price\n",
        "    obs[-1] = self.cash_in_hand\n",
        "    return obs\n",
        "\n",
        "\n",
        "\n",
        "  def _get_val(self):\n",
        "    return self.stock_owned.dot(self.stock_price) + self.cash_in_hand\n",
        "\n",
        "\n",
        "## FILIP comment: when selling sell all, when buying, buy just one share\n",
        "  def _trade(self, action):\n",
        "    # index the action we want to perform\n",
        "    # 0 = sell\n",
        "    # 1 = hold\n",
        "    # 2 = buy\n",
        "    # e.g. [2,1,0] means:\n",
        "    # buy first stock\n",
        "    # hold second stock\n",
        "    # sell third stock\n",
        "    action_vec = self.action_list[action]\n",
        "\n",
        "    # determine which stocks to buy or sell\n",
        "    sell_index = [] # stores index of stocks we want to sell\n",
        "    buy_index = [] # stores index of stocks we want to buy\n",
        "    for i, a in enumerate(action_vec):# i is the counting index: 0,1,2; a is the action; e.g. action_vec = [0,0,1] -> i= 0,1,2 (stock index), a = 0,0,1\n",
        "      if a == 0:\n",
        "        sell_index.append(i)\n",
        "      elif a == 2:\n",
        "        buy_index.append(i)\n",
        "\n",
        "    # sell any stocks we want to sell\n",
        "    # then buy any stocks we want to buy\n",
        "    if sell_index:\n",
        "      # NOTE: to simplify the problem, when we sell, we will sell ALL shares of that stock\n",
        "      for i in sell_index:\n",
        "        self.cash_in_hand += self.stock_price[i] * self.stock_owned[i]\n",
        "        self.stock_owned[i] = 0\n",
        "    if buy_index:\n",
        "      # NOTE: when buying, we will loop through each stock we want to buy,\n",
        "      #       and buy one share at a time until we run out of cash\n",
        "      can_buy = True\n",
        "      while can_buy:\n",
        "        for i in buy_index:\n",
        "          if self.cash_in_hand > self.stock_price[i]:\n",
        "            self.stock_owned[i] += 1 # buy one share\n",
        "            self.cash_in_hand -= self.stock_price[i]\n",
        "          else:\n",
        "            can_buy = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DEiFa9k6Fl2e"
      },
      "outputs": [],
      "source": [
        "class DQNAgent(object):\n",
        "  def __init__(self, state_size, action_size):\n",
        "    self.state_size = state_size # D\n",
        "    self.action_size = action_size # num_action or K = 27 before\n",
        "    self.gamma = 0.95  # discount rate\n",
        "    self.epsilon = 1.0  # exploration rate\n",
        "    self.epsilon_min = 0.01\n",
        "    self.epsilon_decay = 0.995\n",
        "    self.model = LinearModel(state_size, action_size)\n",
        "\n",
        "  def act(self, state):#epsilon-greedy strategy\n",
        "    if np.random.rand() <= self.epsilon:\n",
        "      return np.random.choice(self.action_size)\n",
        "    act_values = self.model.predict(state)\n",
        "    return np.argmax(act_values[0])  # returns action: argmax of a 1 x D vector - [0] because here we have just N = 1 observation: 1 x D output of predict\n",
        "\n",
        "\n",
        "  def train(self, state, action, reward, next_state, done):\n",
        "    if done:\n",
        "      target = reward # terminal state\n",
        "    else:\n",
        "      target = reward + self.gamma * np.amax(self.model.predict(next_state), axis=1) # any other state, this is r + \\gamma Q(s',a')\n",
        "\n",
        "    target_full = self.model.predict(state) # this is Q(s,:)\n",
        "    target_full[0, action] = target # here we consider only Q(s,a)\n",
        "\n",
        "    # Run one training step\n",
        "    self.model.sgd(state, target_full) # gradient descent method: we update only Q(s,a) -> for action a\n",
        "\n",
        "    if self.epsilon > self.epsilon_min:\n",
        "      self.epsilon *= self.epsilon_decay #update of the \\epsilon for learning -> decreasing in time\n",
        "\n",
        "\n",
        "  def load(self, name):\n",
        "    self.model.load_weights(name) # we load W and b\n",
        "\n",
        "\n",
        "  def save(self, name):\n",
        "    self.model.save_weights(name) # we save the new W and b - it is useful if we want to test some particula W and b at specific time\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VGnwde0hFqXZ"
      },
      "outputs": [],
      "source": [
        "def play_one_episode(agent, env, is_train):\n",
        "  # note: after transforming states are already 1xD\n",
        "  # env is used to call the class \"MultiStockEnv\"\n",
        "  state = env.reset()\n",
        "  state = scaler.transform([state])\n",
        "  done = False\n",
        "\n",
        "  while not done:\n",
        "    action = agent.act(state)\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    next_state = scaler.transform([next_state])\n",
        "    if is_train == 'train':\n",
        "      agent.train(state, action, reward, next_state, done)\n",
        "    state = next_state\n",
        "\n",
        "  return info['cur_val']\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "pCMAs7ifLfeM",
        "outputId": "58f51957-b51d-4751-cc26-2440f7a7415b"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'args' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m     57\u001b[0m   t0 \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;66;03m# the time to measure the duration of each episode\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m   val \u001b[38;5;241m=\u001b[39m play_one_episode(agent, env, \u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m     59\u001b[0m   dt \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m t0\n\u001b[0;32m     60\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_episodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, episode end value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, duration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# we print times for each episode\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'args' is not defined"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "  # config\n",
        "  models_folder = 'linear_rl_trader_models'\n",
        "  rewards_folder = 'linear_rl_trader_rewards'\n",
        "  num_episodes = 2000\n",
        "  #batch_size = 32 # unuseful here - batch size for sampling\n",
        "  initial_investment = 20000\n",
        "\n",
        "  # this is to give the \"train\" or \"test\" mode as input when you run in the terminal\n",
        "  #parser = argparse.ArgumentParser()\n",
        "  #parser.add_argument('-m', '--mode', type=str, required=True,\n",
        "                      #help='either \"train\" or \"test\"')\n",
        "  #args = parser.parse_args()\n",
        "\n",
        "  maybe_make_dir(models_folder)\n",
        "  maybe_make_dir(rewards_folder)\n",
        "\n",
        "  data = get_data()\n",
        "  n_timesteps, n_stocks = data.shape\n",
        "\n",
        "  n_train = n_timesteps // 2\n",
        "\n",
        "  train_data = data[:n_train]\n",
        "  test_data = data[n_train:]\n",
        "\n",
        "  env = MultiStockEnv(train_data, initial_investment)\n",
        "  state_size = env.state_dim\n",
        "  action_size = len(env.action_space)\n",
        "  agent = DQNAgent(state_size, action_size)\n",
        "  scaler = get_scaler(env)\n",
        "\n",
        "  # store the final value of the portfolio (end of episode)\n",
        "  portfolio_value = []\n",
        "\n",
        "  a = \"train\"\n",
        "\n",
        "  if a == 'test':\n",
        "    # then load the previous scaler\n",
        "    with open(f'{models_folder}/scaler.pkl', 'rb') as f:\n",
        "      scaler = pickle.load(f)\n",
        "\n",
        "    # here for test we download the scaler -> we use the mean and std estimated on training set!\n",
        "\n",
        "    # remake the env with test data\n",
        "    env = MultiStockEnv(test_data, initial_investment)\n",
        "\n",
        "    # make sure epsilon is not 1!\n",
        "    # no need to run multiple episodes if epsilon = 0, it's deterministic\n",
        "    agent.epsilon = 0.01\n",
        "\n",
        "    # load trained weights\n",
        "    agent.load(f'{models_folder}/linear.npz')\n",
        "\n",
        "  # play the game num_episodes times\n",
        "  for e in range(num_episodes):\n",
        "    t0 = datetime.now() # the time to measure the duration of each episode\n",
        "    val = play_one_episode(agent, env, a)\n",
        "    dt = datetime.now() - t0\n",
        "    print(f\"episode: {e + 1}/{num_episodes}, episode end value: {val:.2f}, duration: {dt}\") # we print times for each episode\n",
        "    portfolio_value.append(val) # append episode end portfolio value\n",
        "\n",
        "  # save the weights when we are done\n",
        "  if a == 'train':\n",
        "    # save the DQN\n",
        "    agent.save(f'{models_folder}/linear.npz')\n",
        "\n",
        "    # save the scaler\n",
        "    with open(f'{models_folder}/scaler.pkl', 'wb') as f:\n",
        "      pickle.dump(scaler, f)\n",
        "\n",
        "    # plot losses\n",
        "    plt.plot(agent.model.losses)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "  # save portfolio value for each episode\n",
        "  np.save(f'{rewards_folder}/{a}.npy', portfolio_value)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
