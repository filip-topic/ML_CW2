{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D58x1WwNJmdh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datetime import datetime\n",
        "import itertools\n",
        "import argparse\n",
        "import re\n",
        "import os\n",
        "import gym\n",
        "import pickle\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from google.colab import drive\n",
        "drive.mount('/content/new_drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWIWGH51Me9f"
      },
      "outputs": [],
      "source": [
        "def identify_market_periods(data, window=30, threshold=0.02):\n",
        "    \"\"\"\n",
        "    Define market periods based on rolling window volatility.\n",
        "    :param data: Historical stock price data\n",
        "    :param window: Rolling window size\n",
        "    :param threshold: Volatility threshold to distinguish stable and crisis periods\n",
        "    :return: Labeled market periods\n",
        "    \"\"\"\n",
        "    returns = data.pct_change()\n",
        "    volatility = returns.rolling(window=window).std()\n",
        "    market_periods = np.where(volatility > threshold, 'crisis', 'stable')\n",
        "    # Flatten the market_periods array to match the index\n",
        "    market_periods = pd.DataFrame(market_periods, index=data.index, columns=data.columns)\n",
        "    return market_periods\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqYNYgzp1hJ2"
      },
      "outputs": [],
      "source": [
        "def get_data(tickers, start, end):\n",
        "    data = yf.download(tickers, start=start, end=end)\n",
        "    return data['Adj Close']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frGXtMIDLcbG"
      },
      "outputs": [],
      "source": [
        "def get_scaler(env):\n",
        "    # return scikit-learn scaler object to scale the states\n",
        "    states = []\n",
        "    for _ in range(env.n_step):\n",
        "        action = env.action_space.sample()  # Use sample() to generate a random action\n",
        "        state, reward, done, info = env.step(action)\n",
        "        states.append(state)\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(states)\n",
        "    return scaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMdf29kbLd8L"
      },
      "outputs": [],
      "source": [
        "def maybe_make_dir(directory):\n",
        "  if not os.path.exists(directory):\n",
        "    os.makedirs(directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def buy_and_hold(data, initial_investment):\n",
        "    # Purchase of all shares of equal value on the first day of the investment period\n",
        "    n_stocks = data.shape[1]\n",
        "    investment_per_stock = initial_investment / n_stocks\n",
        "    first_day_prices = data.iloc[0]\n",
        "    shares_bought = investment_per_stock / first_day_prices\n",
        "\n",
        "    # Calculate the total value on the last day of the investment period\n",
        "    last_day_prices = data.iloc[-1]\n",
        "    final_portfolio_value = (shares_bought * last_day_prices).sum()\n",
        "    return final_portfolio_value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_agent(agent, env, scaler):\n",
        "    state = env.reset()\n",
        "    state = scaler.transform([state])\n",
        "    done = False\n",
        "    while not done:\n",
        "        action_values = agent.act(state)\n",
        "        action = np.clip(action_values, env.action_space.low, env.action_space.high)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        next_state = scaler.transform([next_state])\n",
        "        state = next_state\n",
        "    return info['cur_val']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eok4QrnaLhmg"
      },
      "outputs": [],
      "source": [
        "class LinearModel:\n",
        "    \"\"\" A linear regression model \"\"\"\n",
        "    def __init__(self, input_dim, n_action):\n",
        "        self.W = np.random.randn(input_dim, n_action) / np.sqrt(input_dim)\n",
        "        self.b = np.zeros(n_action)\n",
        "\n",
        "        # momentum terms\n",
        "        self.vW = 0\n",
        "        self.vb = 0\n",
        "\n",
        "        self.losses = []\n",
        "\n",
        "    def predict(self, X):\n",
        "        assert(len(X.shape) == 2)\n",
        "        return X.dot(self.W) + self.b\n",
        "\n",
        "    def sgd(self, X, Y, learning_rate=0.01, momentum=0.9):\n",
        "        assert(len(X.shape) == 2)\n",
        "        num_values = np.prod(Y.shape)\n",
        "\n",
        "        Yhat = self.predict(X)\n",
        "        gW = 2 * X.T.dot(Yhat - Y) / num_values\n",
        "        gb = 2 * (Yhat - Y).sum(axis=0) / num_values\n",
        "\n",
        "        self.vW = momentum * self.vW - learning_rate * gW\n",
        "        self.vb = momentum * self.vb - learning_rate * gb\n",
        "\n",
        "        self.W += self.vW\n",
        "        self.b += self.vb\n",
        "\n",
        "        mse = np.mean((Yhat - Y)**2)\n",
        "        self.losses.append(mse)\n",
        "\n",
        "    def load_weights(self, filepath):\n",
        "        npz = np.load(filepath)\n",
        "        self.W = npz['W']\n",
        "        self.b = npz['b']\n",
        "\n",
        "    def save_weights(self, filepath):\n",
        "        np.savez(filepath, W=self.W, b=self.b)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3zmUK0vLlZ2"
      },
      "outputs": [],
      "source": [
        "class MultiStockEnv:\n",
        "    \"\"\"Multi-stock trading environment\"\"\"\n",
        "    def __init__(self, data, initial_investment=20000, market_periods=None):\n",
        "        self.stock_price_history = data\n",
        "        self.market_periods = market_periods\n",
        "        self.n_step, self.n_stock = self.stock_price_history.shape\n",
        "\n",
        "        self.initial_investment = initial_investment\n",
        "        self.cur_step = None\n",
        "        self.stock_owned = None\n",
        "        self.stock_price = None\n",
        "        self.cash_in_hand = None\n",
        "\n",
        "        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(self.n_stock,), dtype=np.float32)\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=np.inf, shape=(self.n_stock * 2 + 2,), dtype=np.float32)\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.cur_step = 0\n",
        "        self.stock_owned = np.zeros(self.n_stock)\n",
        "        self.stock_price = self.stock_price_history.iloc[self.cur_step].values\n",
        "        self.cash_in_hand = self.initial_investment\n",
        "        return self._get_obs()\n",
        "\n",
        "    def step(self, action):\n",
        "        action = np.clip(action, self.action_space.low, self.action_space.high)\n",
        "        for i, val in enumerate(action):\n",
        "            assert self.action_space.low[i] <= val <= self.action_space.high[i], f\"Action {action} is not valid for the action space {self.action_space}\"\n",
        "\n",
        "        action = (action * self.initial_investment // self.stock_price).astype(int)\n",
        "\n",
        "        prev_val = self._get_val()\n",
        "        self.cur_step += 1\n",
        "        self.stock_price = self.stock_price_history.iloc[self.cur_step].values\n",
        "        self._trade(action)\n",
        "        cur_val = self._get_val()\n",
        "\n",
        "        reward = cur_val - prev_val\n",
        "        done = self.cur_step == self.n_step - 1\n",
        "        info = {'cur_val': cur_val}\n",
        "        return self._get_obs(), reward, done, info\n",
        "\n",
        "    def _get_obs(self):\n",
        "        obs = np.empty(self.n_stock * 2 + 2)\n",
        "        obs[:self.n_stock] = self.stock_owned\n",
        "        obs[self.n_stock:2*self.n_stock] = self.stock_price\n",
        "        obs[-2] = self.cash_in_hand\n",
        "        obs[-1] = 1 if 'crisis' in self.market_periods.iloc[self.cur_step].values else 0\n",
        "        return obs\n",
        "\n",
        "    def _get_val(self):\n",
        "        return self.stock_owned.dot(self.stock_price) + self.cash_in_hand\n",
        "\n",
        "    def _trade(self, action):\n",
        "        sell_index = np.where(action < 0)[0]\n",
        "        buy_index = np.where(action > 0)[0]\n",
        "\n",
        "        for i in sell_index:\n",
        "            self.cash_in_hand += self.stock_price[i] * min(self.stock_owned[i], -action[i])\n",
        "            self.stock_owned[i] -= min(self.stock_owned[i], -action[i])\n",
        "\n",
        "        for i in buy_index:\n",
        "            num_shares_to_buy = min(self.cash_in_hand // self.stock_price[i], action[i])\n",
        "            self.stock_owned[i] += num_shares_to_buy\n",
        "            self.cash_in_hand -= self.stock_price[i] * num_shares_to_buy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VjQF0vBLo8V"
      },
      "outputs": [],
      "source": [
        "class DQNAgent(object):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.gamma = 0.95\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.01\n",
        "        self.epsilon_decay = 0.995\n",
        "        self.model = LinearModel(state_size, action_size)\n",
        "\n",
        "    def act(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return np.random.uniform(low=-1, high=1, size=self.action_size)\n",
        "        act_values = self.model.predict(state)\n",
        "        return np.clip(act_values[0], -1, 1)\n",
        "\n",
        "    def train(self, state, action, reward, next_state, done):\n",
        "        if done:\n",
        "            target = reward\n",
        "        else:\n",
        "            target = reward + self.gamma * np.amax(self.model.predict(next_state), axis=1)\n",
        "\n",
        "        target_full = self.model.predict(state)\n",
        "        action_idx = np.argmax(action)\n",
        "        target_full[0, action_idx] = target\n",
        "\n",
        "        self.model.sgd(state, target_full)\n",
        "\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay\n",
        "\n",
        "    def load(self, name):\n",
        "        self.model.load_weights(name)\n",
        "\n",
        "    def save(self, name):\n",
        "        self.model.save_weights(name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9B_w77qGLs1l"
      },
      "outputs": [],
      "source": [
        "def play_one_episode(agent, env, is_train):\n",
        "    state = env.reset()\n",
        "    state = scaler.transform([state])\n",
        "    done = False\n",
        "    #episode_actions = [] # Used to record the action in each iteration\n",
        "\n",
        "    while not done:\n",
        "        action_values = agent.act(state)\n",
        "        action = np.clip(action_values, env.action_space.low, env.action_space.high)\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        next_state = scaler.transform([next_state])\n",
        "        if is_train == 'train':\n",
        "            agent.train(state, action_values, reward, next_state, done)\n",
        "        state = next_state\n",
        "        #episode_actions.append(action)  # record the action\n",
        "\n",
        "    return info['cur_val']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    models_folder = '/content/new_drive/MyDrive/UCL/ML/linear_rl_trader_models'\n",
        "    rewards_folder = '/content/new_drive/MyDrive/UCL/ML/linear_rl_trader_rewards'\n",
        "    num_episodes = 2000\n",
        "    initial_investment = 20000\n",
        "\n",
        "    maybe_make_dir(models_folder)\n",
        "    maybe_make_dir(rewards_folder)\n",
        "\n",
        "    tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN']\n",
        "    start_date = '2010-01-01'\n",
        "    end_date = '2020-12-31'\n",
        "\n",
        "    data = yf.download(tickers, start=start_date, end=end_date)['Adj Close']\n",
        "\n",
        "    market_periods = identify_market_periods(data)\n",
        "\n",
        "    print(data.head())\n",
        "    print(market_periods.head())\n",
        "    print(f\"Data shape: {data.shape}, Market periods shape: {market_periods.shape}\")\n",
        "\n",
        "    n_timesteps, n_stocks = data.shape\n",
        "    n_train = n_timesteps // 2\n",
        "    train_data = data.iloc[:n_train]\n",
        "    test_data = data.iloc[n_train:]\n",
        "    train_periods = market_periods.iloc[:n_train]\n",
        "    test_periods = market_periods.iloc[n_train:]\n",
        "\n",
        "    env = MultiStockEnv(train_data, initial_investment, train_periods)\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    action_size = env.action_space.shape[0]\n",
        "    agent = DQNAgent(state_size, action_size)\n",
        "    scaler = get_scaler(env)\n",
        "\n",
        "    portfolio_value = []\n",
        "    #all_actions = []  # for recording all actions\n",
        "\n",
        "    for e in range(num_episodes):\n",
        "        t0 = datetime.now()\n",
        "        val = play_one_episode(agent, env, 'train')\n",
        "        dt = datetime.now() - t0\n",
        "        print(f\"episode: {e + 1}/{num_episodes}, episode end value: {val:.2f}, duration: {dt}\")\n",
        "        portfolio_value.append(val)\n",
        "        #all_actions.append(actions) # record all actions in each iteration\n",
        "\n",
        "    if 'train' == 'train':\n",
        "        agent.save(f'{models_folder}/linear.npz')\n",
        "        with open(f'{models_folder}/scaler.pkl', 'wb') as f:\n",
        "            pickle.dump(scaler, f)\n",
        "        plt.plot(agent.model.losses)\n",
        "        plt.show()\n",
        "\n",
        "    np.save(f'{rewards_folder}/train.npy', portfolio_value)\n",
        "\n",
        "\n",
        "    # Save all actions to a file\n",
        "    #with open(f'{rewards_folder}/actions.pkl', 'wb') as f:\n",
        "        #pickle.dump(all_actions, f)\n",
        "    # Print all actions in the last iteration\n",
        "    #print(f\"Actions in the last episode: {all_actions[-1]}\")\n",
        "\n",
        "       # Validation: Compare with Buy and Hold strategy\n",
        "    # Calculate Buy and Hold performance\n",
        "    buy_and_hold_value = buy_and_hold(test_data, initial_investment)\n",
        "    print(f\"Buy and Hold strategy final portfolio value: {buy_and_hold_value:.2f}\")\n",
        "\n",
        "    # Test the trained agent\n",
        "    test_env = MultiStockEnv(test_data, initial_investment, test_periods)\n",
        "    final_portfolio_value = test_agent(agent, test_env, scaler)\n",
        "    print(f\"Trained agent final portfolio value: {final_portfolio_value:.2f}\")\n",
        "\n",
        "    # Compare the results\n",
        "    if final_portfolio_value > buy_and_hold_value:\n",
        "        print(\"The trained agent outperformed the Buy and Hold strategy.\")\n",
        "    else:\n",
        "        print(\"The Buy and Hold strategy outperformed the trained agent.\")\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YlZLHfxVhf8"
      },
      "outputs": [],
      "source": [
        "# View the final iteration's portfolio value at the end of training\n",
        "print(f\"Final portfolio value: {portfolio_value[-1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWcM_ZCTjhRG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
