{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_excel('ftw1002023.xlsx')\n",
    "test_df = pd.read_excel('ft100test.xlsx')\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler()\n",
    "train_data = scaler.fit_transform(train_df[['Close', 'Open', 'Low', 'High', 'Volume', 'Turnover - GBP', 'Flow']])\n",
    "test_data = scaler.transform(test_df[['Close', 'Open', 'Low', 'High', 'Volume', 'Turnover - GBP', 'Flow']])\n",
    "\n",
    "# Set TensorFlow to use all available CPU cores\n",
    "num_cpus = multiprocessing.cpu_count()\n",
    "tf.config.threading.set_intra_op_parallelism_threads(num_cpus)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(num_cpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Actor Network\n",
    "def create_actor(state_size, action_size, hidden_size):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(state_size,)),\n",
    "        layers.Dense(hidden_size, activation='relu'),\n",
    "        layers.Dense(hidden_size, activation='relu'),\n",
    "        layers.Dense(action_size, activation='tanh')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define the Critic Network\n",
    "def create_critic(state_size, action_size, hidden_size):\n",
    "    state_input = layers.Input(shape=(state_size,))\n",
    "    action_input = layers.Input(shape=(action_size,))\n",
    "    concat = layers.Concatenate()([state_input, action_input])\n",
    "    \n",
    "    dense = layers.Dense(hidden_size, activation='relu')(concat)\n",
    "    dense = layers.Dense(hidden_size, activation='relu')(dense)\n",
    "    output = layers.Dense(1)(dense)\n",
    "    \n",
    "    model = models.Model(inputs=[state_input, action_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "# Initialize parameters\n",
    "state_size = train_data.shape[1]\n",
    "action_size = 1  # Buy, Sell, Hold\n",
    "hidden_size = 64\n",
    "actor_lr = 1e-4\n",
    "critic_lr = 1e-3\n",
    "gamma = 0.99\n",
    "tau = 0.001\n",
    "buffer_size = 1000000\n",
    "batch_size = 64\n",
    "\n",
    "# Initialize actor and critic networks\n",
    "actor = create_actor(state_size, action_size, hidden_size)\n",
    "critic = create_critic(state_size, action_size, hidden_size)\n",
    "\n",
    "# Initialize target networks\n",
    "target_actor = create_actor(state_size, action_size, hidden_size)\n",
    "target_critic = create_critic(state_size, action_size, hidden_size)\n",
    "target_actor.set_weights(actor.get_weights())\n",
    "target_critic.set_weights(critic.get_weights())\n",
    "\n",
    "# Define optimizers\n",
    "actor_optimizer = optimizers.Adam(learning_rate=actor_lr)\n",
    "critic_optimizer = optimizers.Adam(learning_rate=critic_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size):\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.buffer, k=self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        \n",
    "        states = np.array(states)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards).reshape(-1, 1)\n",
    "        next_states = np.array(next_states)\n",
    "        dones = np.array(dones).reshape(-1, 1)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Initialize replay buffer\n",
    "replay_buffer = ReplayBuffer(buffer_size, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to update target networks\n",
    "def soft_update(local_model, target_model, tau):\n",
    "    local_weights = np.array(local_model.get_weights())\n",
    "    target_weights = np.array(target_model.get_weights())\n",
    "    new_weights = tau * local_weights + (1.0 - tau) * target_weights\n",
    "    target_model.set_weights(new_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tc/pdvvjy9x3pxgynb3mjz4nw9r0000gn/T/ipykernel_27132/2591618807.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  local_weights = np.array(local_model.get_weights())\n",
      "/var/folders/tc/pdvvjy9x3pxgynb3mjz4nw9r0000gn/T/ipykernel_27132/2591618807.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  target_weights = np.array(target_model.get_weights())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0/100, Actor Loss: -0.12531980872154236, Critic Loss: 0.000376245123334229\n",
      "Episode 10/100, Actor Loss: -0.12905961275100708, Critic Loss: 0.0007286440813913941\n",
      "Episode 20/100, Actor Loss: -0.10035770386457443, Critic Loss: 0.0002812808961607516\n",
      "Episode 30/100, Actor Loss: -0.08590079098939896, Critic Loss: 0.00015672999143134803\n",
      "Episode 40/100, Actor Loss: -0.060856305062770844, Critic Loss: 0.00020405274699442089\n",
      "Episode 50/100, Actor Loss: -0.05560988187789917, Critic Loss: 0.0001009659463306889\n",
      "Episode 60/100, Actor Loss: -0.04375579208135605, Critic Loss: 0.00011749286932172254\n",
      "Episode 70/100, Actor Loss: -0.02192344143986702, Critic Loss: 8.865556446835399e-05\n",
      "Episode 80/100, Actor Loss: -0.018577896058559418, Critic Loss: 0.0001637219829717651\n",
      "Episode 90/100, Actor Loss: -0.021822737529873848, Critic Loss: 0.0001445497910026461\n"
     ]
    }
   ],
   "source": [
    "def train_ddpg_agent(train_data, num_episodes=100):\n",
    "    for episode in range(num_episodes):\n",
    "        state = train_data[0]  # Initial state\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        \n",
    "        for t in range(1, len(train_data)):\n",
    "            state_tensor = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "            action = actor(state_tensor)\n",
    "            action = action.numpy()\n",
    "            action = np.clip(action, -1, 1)\n",
    "            \n",
    "            next_state = train_data[t]\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            \n",
    "            reward = next_state[0, 0] - state[0, 0]\n",
    "            done = t == (len(train_data) - 1)\n",
    "            \n",
    "            replay_buffer.add(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if len(replay_buffer) > batch_size:\n",
    "                experiences = replay_buffer.sample()\n",
    "                states, actions, rewards, next_states, dones = experiences\n",
    "                \n",
    "                # Ensure states and next_states are 2D tensors with shape (batch_size, state_size)\n",
    "                states = np.reshape(states, (batch_size, state_size))\n",
    "                next_states = np.reshape(next_states, (batch_size, state_size))\n",
    "                \n",
    "                # Ensure actions are 2D tensors with shape (batch_size, action_size)\n",
    "                actions = np.reshape(actions, (batch_size, action_size))\n",
    "                \n",
    "                # Convert to tensors\n",
    "                states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "                next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "                actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "                rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "                dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "                \n",
    "                next_actions = target_actor(next_states)\n",
    "                \n",
    "                # Compute the target Q values\n",
    "                target_q_values = target_critic([next_states, next_actions])\n",
    "                target_q_values = rewards + (gamma * target_q_values * (1 - dones))\n",
    "                \n",
    "                with tf.GradientTape() as tape:\n",
    "                    expected_q_values = critic([states, actions])\n",
    "                    critic_loss = tf.reduce_mean(tf.square(expected_q_values - target_q_values))\n",
    "                critic_grads = tape.gradient(critic_loss, critic.trainable_variables)\n",
    "                critic_optimizer.apply_gradients(zip(critic_grads, critic.trainable_variables))\n",
    "                \n",
    "                with tf.GradientTape() as tape:\n",
    "                    predicted_actions = actor(states)\n",
    "                    actor_loss = -tf.reduce_mean(critic([states, predicted_actions]))\n",
    "                actor_grads = tape.gradient(actor_loss, actor.trainable_variables)\n",
    "                actor_optimizer.apply_gradients(zip(actor_grads, actor.trainable_variables))\n",
    "                \n",
    "                soft_update(actor, target_actor, tau)\n",
    "                soft_update(critic, target_critic, tau)\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            print(f'Episode {episode}/{num_episodes}, Actor Loss: {actor_loss.numpy()}, Critic Loss: {critic_loss.numpy()}')\n",
    "\n",
    "# Train the DDPG agent on the training data\n",
    "train_ddpg_agent(train_data, num_episodes=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: 0.1633237438053663, Final Portfolio Value: 1.1698804939428415\n",
      "Buy and Hold Return: 0.8505684595451913\n",
      "DDPG Strategy Final Portfolio Value: 1.1698804939428415\n",
      "Buy and Hold Strategy Final Portfolio Value: 0.8505684595451913\n"
     ]
    }
   ],
   "source": [
    "# Backtest the DDPG agent\n",
    "def backtest_ddpg_agent(test_data, actor):\n",
    "    state = test_data[0]\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "    portfolio_value = 1  # Starting with an initial portfolio value of 1\n",
    "\n",
    "    for t in range(1, len(test_data)):\n",
    "        state_tensor = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "        action = actor(state_tensor)\n",
    "        action = action.numpy()\n",
    "        action = np.clip(action, -1, 1)\n",
    "\n",
    "        next_state = test_data[t]\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "        # Simulate trading: Buy (1), Sell (-1), or Hold (0)\n",
    "        reward = (next_state[0, 0] - state[0, 0]) * action[0, 0]  # Simplified reward: change in 'Close' price\n",
    "        total_reward += reward\n",
    "        portfolio_value *= (1 + reward)  # Update portfolio value\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    return total_reward, portfolio_value\n",
    "\n",
    "total_reward, portfolio_value = backtest_ddpg_agent(test_data, actor)\n",
    "print(f\"Total Reward: {total_reward}, Final Portfolio Value: {portfolio_value}\")\n",
    "\n",
    "# Evaluate buy-and-hold strategy\n",
    "def buy_and_hold(test_data):\n",
    "    initial_price = test_data[0, 0]\n",
    "    final_price = test_data[-1, 0]\n",
    "    return final_price / initial_price\n",
    "\n",
    "buy_and_hold_return = buy_and_hold(test_data)\n",
    "print(f\"Buy and Hold Return: {buy_and_hold_return}\")\n",
    "\n",
    "# Compare the two strategies\n",
    "print(f\"DDPG Strategy Final Portfolio Value: {portfolio_value}\")\n",
    "print(f\"Buy and Hold Strategy Final Portfolio Value: {buy_and_hold_return}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
